{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle as pk\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression,Lasso,Ridge\n",
    "from sklearn.preprocessing import StandardScaler,QuantileTransformer,PolynomialFeatures,KBinsDiscretizer,OneHotEncoder\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = \"alert\"\n",
    "assert baseline in [\"alert\",\"admission\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open(f\"/edata/clarity_extracts/processed_features/outcome_analysis_features_{baseline}.pkl\",'rb') as f:\n",
    "    data = pk.load(f)\n",
    "    \n",
    "# rename some features\n",
    "data.rename({'time_of_discharge':'discharge_tsp','sex':'gender'},axis=1,inplace=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load alert confirmation data and merge with features\n",
    "with open(f\"/edata/clarity_extracts/processed_features/confirmations.pkl\",'rb') as f:\n",
    "    confirmations = pk.load(f)\n",
    "data = data.merge(confirmations,how='left')\n",
    "\n",
    "# Load comorbidity data and merge with features\n",
    "with open(f\"/edata/clarity_extracts/processed_features/mimics.pkl\",'rb') as f:\n",
    "    mimics = pk.load(f)    \n",
    "data = data.merge(mimics,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique hospital names\n",
    "hospitals = data.hospital.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precalc some temporal features\n",
    "data[\"hours_admit_to_discharge\"] = (data.discharge_tsp - data.admission_tsp)/pd.Timedelta(1,'h')\n",
    "data[\"hours_alert_to_discharge\"] = (data.discharge_tsp - data.alert_tsp)/pd.Timedelta(1,'h')\n",
    "\n",
    "data[\"hours_admission_to_abx\"] = data.hours_admit_to_abx\n",
    "data['hours_admission_to_alert'] = (data.alert_tsp - data.admission_tsp)/pd.Timedelta(1,'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre compute time to confirmation\n",
    "data['hours_alert_to_confirmation'] = (pd.to_datetime(data.confirmed_tsp,utc=True) - data.alert_tsp)/pd.Timedelta(1,'h')\n",
    "data['confirmed_in_3_hrs'] = 1*(data.hours_alert_to_confirmation <= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hospital indicators\n",
    "ds_cols = []\n",
    "for h in [\"HCGH\",\"BMC\",\"JHH\",\"SH\",\"SMH\"]:\n",
    "    data[h.lower()] = 1*(data.hospital == h)\n",
    "    ds_cols.append(h.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create post-covid indicator\n",
    "data[\"post_covid\"] = 1\n",
    "data.loc[data.admission_tsp < pd.Timestamp(year=2020,month=4,day=1,tz='UTC'),\"post_covid\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get previous admissions (used for sensitivity analysis)\n",
    "pat_enc = pd.read_csv('/edata/clarity_extracts/cdm/pat_enc.csv')\n",
    "data = data.merge(pat_enc[['enc_id','pat_id']],how='left')\n",
    "data.sort_values('admission_tsp',inplace=True)\n",
    "data['prev_admission_tsp'] = data.groupby('pat_id').admission_tsp.shift(periods=1)\n",
    "data['prev_admission'] = 1*(~data.prev_admission_tsp.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create post deployment mask\n",
    "deploy_dates = {\n",
    "    \"HCGH\":pd.Timestamp(year=2018,month=4,day=1,tz=\"UTC\"),\n",
    "    \"BMC\":pd.Timestamp(year=2019,month=2,day=27,tz=\"UTC\"),\n",
    "    \"SH\":pd.Timestamp(year=2018,month=10,day=10,tz=\"UTC\"),\n",
    "    \"JHH\":pd.Timestamp(year=2019,month=4,day=16,tz=\"UTC\"),\n",
    "    \"SMH\":pd.Timestamp(year=2019,month=5,day=15,tz=\"UTC\")\n",
    "}\n",
    "post_mask = (data.hospital == \"HCGH\") & (data.admission_tsp >= deploy_dates['HCGH'])\n",
    "post_mask = post_mask | ((data.hospital == \"BMC\") & (data.admission_tsp >= deploy_dates['BMC']))\n",
    "post_mask = post_mask | ((data.hospital == \"SH\") & (data.admission_tsp >= deploy_dates['SH']))\n",
    "post_mask = post_mask | ((data.hospital == \"JHH\") & (data.admission_tsp >= deploy_dates['JHH']))\n",
    "post_mask = post_mask | ((data.hospital == \"SMH\") & (data.admission_tsp >= deploy_dates['SMH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for processing features\n",
    "def get_features(data,filt,tta_col,feature_cols,ftype='mean',normalize=True):\n",
    "    data_trews = data[filt]\n",
    "    N = data_trews.shape[0]\n",
    "    print(\"N = %d\"%N)\n",
    "\n",
    "\n",
    "    T = data_trews[tta_col].values.reshape((N,1))\n",
    "    # S = data_trews[\"septic_shock\"].values.reshape((N,1))\n",
    "    X = data_trews[feature_cols].values.copy()\n",
    "    age = data_trews[\"age\"].values\n",
    "\n",
    "    X_aug = []\n",
    "    feature_cols_aug = []\n",
    "    for c in range(X.shape[1]):\n",
    "        nan_mask = np.isnan(X[:,c])\n",
    "        if np.all((X[~nan_mask,c] == 0)|(X[~nan_mask,c]==1)):\n",
    "            X[nan_mask,c] = 0\n",
    "        else:\n",
    "            mu = X[~nan_mask,c].mean()\n",
    "            X[nan_mask,c] = mu\n",
    "\n",
    "        if feature_cols[c].endswith(\"_histogram\"):\n",
    "            fid = feature_cols[c][:-10]\n",
    "            fid_col = fid + f\"_{ftype}\" if fid != \"age\" else \"age\"\n",
    "            \n",
    "            X_c_aug = OneHotEncoder(sparse=False).fit_transform(X[:,c:c+1])\n",
    "            feat = data_trews[fid_col].values.reshape((N,1))\n",
    "            mu = np.nanmean(feat)\n",
    "            if fid != 'gcs':\n",
    "                feat[np.isnan(feat)] = mu\n",
    "            else:\n",
    "                feat[np.isnan(feat)] = 15\n",
    "            if normalize:\n",
    "                feat = (feat - mu)/np.std(feat)\n",
    "            X_aug.append(X_c_aug * feat)\n",
    "            feature_cols_aug += [\"%s_%d\"%(feature_cols[c],v) for v in range(X_c_aug.shape[1])]\n",
    "            \n",
    "        else:\n",
    "            feat = X[:,c:c+1].copy()\n",
    "            binary_feature = np.all((X[~nan_mask,c] == 0)|(X[~nan_mask,c]==1))\n",
    "            if (not binary_feature) and normalize:\n",
    "                feat = (feat - mu)/np.std(feat)\n",
    "            X_aug.append(feat)\n",
    "            feature_cols_aug.append(feature_cols[c])\n",
    "    \n",
    "    X = np.hstack([np.ones((N,1))] + X_aug).copy()\n",
    "    feature_cols_aug.insert(0,\"const\")\n",
    "    print(\"n_features = %d\"%X.shape[1])\n",
    "    Y = 1*((data_trews.death_in_hospital.values==1))\n",
    "    \n",
    "    return X,T,Y,feature_cols_aug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns\n",
    "feature_cols = [\n",
    "    \"sofa_resp\",\n",
    "    \"sofa_card\",\n",
    "    \"sofa_kidn\",\n",
    "    \"sofa_live\",\n",
    "    \"sofa_nerv\",\n",
    "    \"sofa_coag\",\n",
    "    \"apache2\",\n",
    "    \"cci\",\n",
    "    \"gender\",\n",
    "    \"dementia\", # Harder to recognize sepsis\n",
    "    \"diabetes_wo_comp\", # mild immumocompromised\n",
    "    \"diabetes_w_comp\", # mild immumocompromised\n",
    "    \"malignancy\", # immunocompromised\n",
    "    \"met_solid_tumor\", # immunocompromised\n",
    "    \"abnormal_gcs\",\n",
    "    \"lactate_ind\",\n",
    "    \"age_histogram\",\n",
    "    \"bp_sys_histogram\",\n",
    "    \"heart_rate_histogram\",\n",
    "    \"resp_rate_histogram\",\n",
    "    \"wbc_histogram\",\n",
    "    'temperature_histogram',\n",
    "    'gcs_histogram',\n",
    "    'vasopressors',\n",
    "    'mech_vent',\n",
    "    'post_covid'\n",
    "                ]\n",
    "\n",
    "aux_features = [\n",
    "    'cci_met_solid_tumor_prob_poa',\n",
    "    'esrd_prob_poa',\n",
    "    'cci_chf_prob_poa',\n",
    "    'acute_liver_disease_prob_poa',\n",
    "    'gi_bleed_prob_poa',\n",
    "    'copd_prob_poa',\n",
    "    'trauma_admit'\n",
    "               ]\n",
    "\n",
    "final_diag_features = [\n",
    "    'cci_met_solid_tumor_final_diag',\n",
    "    'esrd_final_diag',\n",
    "    'cci_chf_final_diag',\n",
    "    'acute_liver_disease_final_diag',\n",
    "    'gi_bleed_final_diag',\n",
    "    'copd_final_diag',\n",
    "    'trauma_admit'\n",
    "               ]\n",
    "\n",
    "hospitals_in_data = np.char.lower(data.hospital.unique().astype(str))\n",
    "feature_cols = feature_cols + sorted(list(hospitals_in_data))[1:]\n",
    "\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train risk model\n",
    "Using retrospective data, train a model that predicts a patients increase in mortality risk with and without prompt antibiotic therapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load retrospective data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"/edata/clarity_extracts/processed_features/retrospective_outcome_analysis_features_{baseline}.pkl\",'rb') as f:\n",
    "    retro_data = pk.load(f)\n",
    "print(retro_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retro_data[\"hours_admit_to_discharge\"] = (retro_data.discharge_tsp - retro_data.admission_tsp)/pd.Timedelta(1,'h')\n",
    "retro_data[\"hours_alert_to_discharge\"] = (retro_data.discharge_tsp - retro_data.alert_tsp)/pd.Timedelta(1,'h')\n",
    "\n",
    "retro_data[\"hours_admission_to_abx\"] = retro_data.hours_admit_to_abx\n",
    "retro_data['hours_admission_to_alert'] = (retro_data.alert_tsp - retro_data.admission_tsp)/pd.Timedelta(1,'h')\n",
    "\n",
    "retro_data['post_covid'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_cols = []\n",
    "for h in [\"HCGH\",\"BMC\",\"JHH\",\"SH\",\"SMH\"]:\n",
    "    retro_data[h.lower()] = 1*(retro_data.hospital == h)\n",
    "    ds_cols.append(h.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define retrospective inclusion criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retro_filt = (retro_data[\"admission_tsp\"] >= pd.Timestamp(year=2016,month=1,day=1,tz='UTC'))\n",
    "retro_filt = retro_filt & (retro_data[\"admission_tsp\"] < pd.Timestamp(year=2018,month=4,day=1,tz='UTC'))\n",
    "\n",
    "# 1. Had an alert\n",
    "retro_filt = retro_filt & (retro_data['hours_admission_to_alert'] > -1)\n",
    "retro_filt = retro_filt & (retro_data[\"alert\"] == 1)\n",
    "print(f\"{np.sum(retro_filt)} patients had an alert\")\n",
    "\n",
    "# 2. Had sepsis\n",
    "retro_filt = retro_filt & (retro_data[\"esp\"] == 1)\n",
    "print(f\"{np.sum(retro_filt)} patients had sepsis\")\n",
    "\n",
    "# 3. Received antibiotics 0 to 24 hours after alert\n",
    "retro_filt = retro_filt & (retro_data.hours_alert_to_abx >  0)\n",
    "print(f\"{np.sum(retro_filt)} patients received abx after alert\")\n",
    "\n",
    "retro_filt = retro_filt & (retro_data.hours_alert_to_abx <= 24)\n",
    "print(f\"{np.sum(retro_filt)} patients received w/in 24 hrs of alert\")\n",
    "\n",
    "retro_filt = retro_filt & (retro_data[\"admit_unit_type\"] != \"icu\")\n",
    "\n",
    "print(f\"{np.sum(retro_filt)} patients met all inclusion criteria\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get features and train retrospective risk model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_col = \"hours_alert_to_abx\"\n",
    "X_retro,T_retro,Y_retro,feature_cols_retro = get_features(retro_data,retro_filt,tta_col,feature_cols,ftype='first')\n",
    "print(X_retro.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import KFold,ShuffleSplit,GridSearchCV\n",
    "\n",
    "dfeats = ['cci','sofa','apache2']\n",
    "feat_kbd = KBinsDiscretizer(n_bins=3,strategy='uniform',encode='onehot-dense').fit(retro_data[retro_filt][dfeats])\n",
    "disc_retro = feat_kbd.transform(retro_data[retro_filt][dfeats])\n",
    "n_int_features = disc_retro.shape[1]\n",
    "n_features = X_retro.shape[1] + n_int_features\n",
    "\n",
    "# Select hyperparameters based on the learned score's\n",
    "# ability to stratify the risk ratio in a held out \n",
    "# dataset\n",
    "def val_obj(mdl,X,y):\n",
    "    N = X.shape[0]\n",
    "    F = n_features\n",
    "    Xf = X[:,:F]\n",
    "    \n",
    "    ts = [1,12]\n",
    "    s = np.zeros((2,Xf.shape[0]))\n",
    "    for t in range(2):\n",
    "        T_test = t*np.ones((N,1))\n",
    "        XT_test = np.hstack((Xf,Xf*T_test))\n",
    "        s[t] = mdl.predict_proba(XT_test)[:,1]\n",
    "    \n",
    "    retro_scores = 1 - s[0]/s[1]\n",
    "    rf = KBinsDiscretizer(3,encode='onehot-dense').fit_transform(retro_scores.reshape((N,1)))\n",
    "    A = X[:,F:F+1]\n",
    "    Xs = np.hstack([Xf,A*rf])\n",
    "    rg_mask = rf[:,-1]==1\n",
    "    XA = X[:,:F+1]\n",
    "    lr = LogisticRegression(C=1000,solver='liblinear').fit(XA[rg_mask],y[rg_mask])\n",
    "    \n",
    "    res = []\n",
    "    for a in range(2):\n",
    "        X_aug = XA.copy()\n",
    "        X_aug[:,-1] = a\n",
    "        res.append(np.mean(lr.predict_proba(X_aug[rg_mask])[:,1]))\n",
    "        \n",
    "    \n",
    "    return 1-(res[0]/res[1])\n",
    "\n",
    "A_retro = 1*(T_retro > 3)\n",
    "\n",
    "XT_retro = np.hstack((X_retro,disc_retro,A_retro*X_retro,disc_retro*A_retro))\n",
    "\n",
    "lr = LogisticRegression(C=0.1,penalty='l2',solver='liblinear')\n",
    "param_grid = {\"C\":[1000,100,10,1,0.1,0.01,0.001]}\n",
    "\n",
    "\n",
    "kf = ShuffleSplit(20,test_size=0.25,random_state=4)\n",
    "mort_model = GridSearchCV(lr,param_grid,cv=kf,scoring=val_obj).fit(XT_retro,Y_retro)\n",
    "print(mort_model.best_params_)\n",
    "\n",
    "ts = [1,12]\n",
    "s = np.zeros((2,X_retro.shape[0]))\n",
    "for t in range(2):\n",
    "    T_test_retro = t*np.ones(T_retro.shape)\n",
    "    XT_test_retro = np.hstack((X_retro,disc_retro,X_retro*T_test_retro,disc_retro*T_test_retro))\n",
    "    s[t] = mort_model.predict_proba(XT_test_retro)[:,1]\n",
    "\n",
    "retro_scores = 1 - s[0]/s[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation scores\n",
    "pd.DataFrame(mort_model.cv_results_)[['param_C','mean_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of risk scores\n",
    "sns.displot(retro_scores)\n",
    "plt.show()\n",
    "\n",
    "# Plot APACHE II vs risk score\n",
    "plt.scatter(retro_data[retro_filt].apache2,retro_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate risk groups on retrospective data\n",
    "nrg = 3\n",
    "kbd = KBinsDiscretizer(n_bins=nrg,encode='onehot-dense').fit(retro_scores.reshape((X_retro.shape[0],1)))\n",
    "risk_features_retro = kbd.transform(retro_scores.reshape((X_retro.shape[0],1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define various inclusion criteria sets for deployment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inclusion_criteria = []\n",
    "\n",
    "inclusion_criteria_names = []\n",
    "\n",
    "feature_sets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = post_mask\n",
    "\n",
    "filt = filt & (data[\"admission_tsp\"] < pd.Timestamp(year=2020,month=10,day=1,tz='UTC'))\n",
    "\n",
    "# 1. Had an alert\n",
    "filt = filt & (data[\"alert\"] == 1)\n",
    "print(f\"{np.sum(filt)} patients had an alert\")\n",
    "\n",
    "# 2. Had sepsis?\n",
    "filt = filt & (data[\"sepsis\"] == 1)\n",
    "print(f\"{np.sum(filt)} patients had sepsis\")\n",
    "\n",
    "print(f'[{(data[filt][\"admit_unit_type\"] == \"icu\").sum()} admitted to icu]')\n",
    "\n",
    "filt = filt & (data['hours_admission_to_alert'] > -1)\n",
    "print(f\"{np.sum(filt)} patients had alert post admission\")\n",
    "prev_sum = np.sum(filt)\n",
    "\n",
    "# 3. Received antibiotics 0 to 24 hours after alert\n",
    "filt = filt & ~(data.hours_alert_to_abx <  0)\n",
    "cur_sum = np.sum(filt)\n",
    "print(f\"{np.sum(filt)} patients received abx after alert ({prev_sum-cur_sum} excluded)\")\n",
    "prev_sum = cur_sum\n",
    "\n",
    "filt = filt & (data.hours_alert_to_abx <= 24)\n",
    "cur_sum = np.sum(filt)\n",
    "print(f\"{np.sum(filt)} patients received w/in 24 hrs of alert ({prev_sum-cur_sum} excluded)\")\n",
    "prev_sum = cur_sum\n",
    "\n",
    "filt = filt & (data[\"admit_unit_type\"] != \"icu\")\n",
    "\n",
    "cur_sum = np.sum(filt)\n",
    "print(f\"{np.sum(filt)} patients met all inclusion criteria ({prev_sum-cur_sum} excluded)\")\n",
    "prev_sum = cur_sum\n",
    "\n",
    "inclusion_criteria.append(filt)\n",
    "inclusion_criteria_names.append('all')\n",
    "feature_sets.append(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = post_mask\n",
    "\n",
    "filt = filt & (data[\"admission_tsp\"] < pd.Timestamp(year=2020,month=10,day=1,tz='UTC'))\n",
    "\n",
    "# 1. Had an alert\n",
    "filt = filt & (data[\"alert\"] == 1)\n",
    "print(f\"{np.sum(filt)} patients had an alert\")\n",
    "\n",
    "# 2. Had sepsis?\n",
    "filt = filt & (data[\"sepsis\"] == 1)\n",
    "print(f\"{np.sum(filt)} patients had sepsis\")\n",
    "\n",
    "print(f'[{(data[filt][\"admit_unit_type\"] == \"icu\").sum()} admitted to icu]')\n",
    "\n",
    "filt = filt & (data['hours_admission_to_alert'] > -1)\n",
    "print(f\"{np.sum(filt)} patients had alert post admission\")\n",
    "prev_sum = np.sum(filt)\n",
    "\n",
    "# 3. Received antibiotics 0 to 24 hours after alert\n",
    "filt = filt & ~(data.hours_alert_to_abx <  0)\n",
    "cur_sum = np.sum(filt)\n",
    "print(f\"{np.sum(filt)} patients received abx after alert ({prev_sum-cur_sum} excluded)\")\n",
    "prev_sum = cur_sum\n",
    "\n",
    "filt = filt & (data.hours_alert_to_abx <= 24)\n",
    "cur_sum = np.sum(filt)\n",
    "print(f\"{np.sum(filt)} patients received w/in 24 hrs of alert ({prev_sum-cur_sum} excluded)\")\n",
    "prev_sum = cur_sum\n",
    "\n",
    "filt = filt & (data[\"admit_unit_type\"] != \"icu\")\n",
    "\n",
    "cur_sum = np.sum(filt)\n",
    "print(f\"{np.sum(filt)} patients met all inclusion criteria ({prev_sum-cur_sum} excluded)\")\n",
    "prev_sum = cur_sum\n",
    "\n",
    "inclusion_criteria.append(filt)\n",
    "inclusion_criteria_names.append('unique_patients')\n",
    "feature_sets.append(feature_cols + aux_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = post_mask\n",
    "\n",
    "# 1. Pre-COVID\n",
    "filt = filt & (data[\"admission_tsp\"] < pd.Timestamp(year=2020,month=4,day=1,tz='UTC'))\n",
    "\n",
    "# 2. Had an alert\n",
    "filt = filt & (data['hours_admission_to_alert'] > -1)\n",
    "filt = filt & (data[\"alert\"] == 1)\n",
    "print(f\"{np.sum(filt)} patients had an alert\")\n",
    "\n",
    "# 3. Had sepsis?\n",
    "filt = filt & (data[\"sepsis\"] == 1)\n",
    "print(f\"{np.sum(filt)} patients had sepsis\")\n",
    "\n",
    "# 4. Received antibiotics 0 to 24 hours after alert\n",
    "filt = filt & (data.hours_alert_to_abx >  0)\n",
    "print(f\"{np.sum(filt)} patients received abx after alert\")\n",
    "\n",
    "filt = filt & (data.hours_alert_to_abx <= 24)\n",
    "print(f\"{np.sum(filt)} patients received w/in 24 hrs of alert\")\n",
    "\n",
    "filt = filt & (data[\"admit_unit_type\"] != \"icu\")\n",
    "\n",
    "print(f\"{np.sum(filt)} patients met all inclusion criteria\")\n",
    "\n",
    "inclusion_criteria.append(filt)\n",
    "inclusion_criteria_names.append('pre_covid')\n",
    "feature_sets.append(feature_cols + aux_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = post_mask\n",
    "\n",
    "filt = filt & (data[\"admission_tsp\"] < pd.Timestamp(year=2020,month=10,day=1,tz='UTC'))\n",
    "\n",
    "# 1. Had an alert\n",
    "filt = filt & (data[\"alert\"] == 1)\n",
    "print(f\"{np.sum(filt)} patients had an alert\")\n",
    "\n",
    "# 2. Had sepsis?\n",
    "filt = filt & (data[\"sepsis\"] == 1)\n",
    "print(f\"{np.sum(filt)} patients had sepsis\")\n",
    "\n",
    "print(f'[{(data[filt][\"admit_unit_type\"] == \"icu\").sum()} admitted to icu]')\n",
    "\n",
    "filt = filt & (data['hours_admission_to_alert'] > -1)\n",
    "print(f\"{np.sum(filt)} patients had alert post admission\")\n",
    "prev_sum = np.sum(filt)\n",
    "\n",
    "# 3. Received antibiotics 0 to 24 hours after alert\n",
    "filt = filt & ~(data.hours_alert_to_abx <  0)\n",
    "cur_sum = np.sum(filt)\n",
    "print(f\"{np.sum(filt)} patients received abx after alert ({prev_sum-cur_sum} excluded)\")\n",
    "prev_sum = cur_sum\n",
    "\n",
    "filt = filt & (data.hours_alert_to_abx <= 24)\n",
    "cur_sum = np.sum(filt)\n",
    "print(f\"{np.sum(filt)} patients received w/in 24 hrs of alert ({prev_sum-cur_sum} excluded)\")\n",
    "prev_sum = cur_sum\n",
    "\n",
    "filt = filt & (data[\"admit_unit_type\"] != \"icu\")\n",
    "\n",
    "cur_sum = np.sum(filt)\n",
    "print(f\"{np.sum(filt)} patients met all inclusion criteria ({prev_sum-cur_sum} excluded)\")\n",
    "prev_sum = cur_sum\n",
    "\n",
    "inclusion_criteria.append(filt)\n",
    "inclusion_criteria_names.append('all_w_mimics')\n",
    "feature_sets.append(feature_cols + aux_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = post_mask\n",
    "\n",
    "filt = filt & (data[\"admission_tsp\"] < pd.Timestamp(year=2020,month=10,day=1,tz='UTC'))\n",
    "\n",
    "# 1. Had an alert\n",
    "filt = filt & (data[\"alert\"] == 1)\n",
    "print(f\"{np.sum(filt)} patients had an alert\")\n",
    "\n",
    "# 2. Had sepsis?\n",
    "filt = filt & (data[\"sepsis\"] == 1)\n",
    "print(f\"{np.sum(filt)} patients had sepsis\")\n",
    "\n",
    "print(f'[{(data[filt][\"admit_unit_type\"] == \"icu\").sum()} admitted to icu]')\n",
    "\n",
    "filt = filt & (data['hours_admission_to_alert'] > -1)\n",
    "print(f\"{np.sum(filt)} patients had alert post admission\")\n",
    "prev_sum = np.sum(filt)\n",
    "\n",
    "# 3. Received antibiotics 0 to 24 hours after alert\n",
    "filt = filt & ~(data.hours_alert_to_abx <  0)\n",
    "cur_sum = np.sum(filt)\n",
    "print(f\"{np.sum(filt)} patients received abx after alert ({prev_sum-cur_sum} excluded)\")\n",
    "prev_sum = cur_sum\n",
    "\n",
    "filt = filt & (data.hours_alert_to_abx <= 24)\n",
    "cur_sum = np.sum(filt)\n",
    "print(f\"{np.sum(filt)} patients received w/in 24 hrs of alert ({prev_sum-cur_sum} excluded)\")\n",
    "prev_sum = cur_sum\n",
    "\n",
    "filt = filt & (data[\"admit_unit_type\"] != \"icu\")\n",
    "\n",
    "cur_sum = np.sum(filt)\n",
    "print(f\"{np.sum(filt)} patients met all inclusion criteria ({prev_sum-cur_sum} excluded)\")\n",
    "prev_sum = cur_sum\n",
    "\n",
    "inclusion_criteria.append(filt)\n",
    "inclusion_criteria_names.append('all_w_final_diags')\n",
    "feature_sets.append(feature_cols + final_diag_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filt = post_mask\n",
    "\n",
    "filt = filt & (data[\"admission_tsp\"] < pd.Timestamp(year=2020,month=10,day=1,tz='UTC'))\n",
    "\n",
    "# 1. Had an alert\n",
    "filt = filt & (data[\"alert\"] == 1)\n",
    "print(f\"{np.sum(filt)} patients had an alert\")\n",
    "\n",
    "# 2. Had sepsis?\n",
    "filt = filt & (data[\"sepsis\"] == 1)\n",
    "print(f\"{np.sum(filt)} patients had sepsis\")\n",
    "\n",
    "print(f'[{(data[filt][\"admit_unit_type\"] == \"icu\").sum()} admitted to icu]')\n",
    "\n",
    "filt = filt & (data['hours_admission_to_alert'] > -1)\n",
    "print(f\"{np.sum(filt)} patients had alert post admission\")\n",
    "prev_sum = np.sum(filt)\n",
    "\n",
    "# 3. Received antibiotics 0 to 24 hours after alert\n",
    "filt = filt & ~(data.hours_alert_to_abx <  0)\n",
    "cur_sum = np.sum(filt)\n",
    "print(f\"{np.sum(filt)} patients received abx after alert ({prev_sum-cur_sum} excluded)\")\n",
    "prev_sum = cur_sum\n",
    "\n",
    "filt = filt & (data.hours_alert_to_abx <= 24)\n",
    "cur_sum = np.sum(filt)\n",
    "print(f\"{np.sum(filt)} patients received w/in 24 hrs of alert ({prev_sum-cur_sum} excluded)\")\n",
    "prev_sum = cur_sum\n",
    "\n",
    "filt = filt & (data[\"admit_unit_type\"] != \"icu\")\n",
    "\n",
    "\n",
    "filt = filt & ~((data.hours_alert_to_abx <= 3) & (~(data.hours_alert_to_confirmation <= 3)))\n",
    "\n",
    "cur_sum = np.sum(filt)\n",
    "print(f\"{np.sum(filt)} patients met all inclusion criteria ({prev_sum-cur_sum} excluded)\")\n",
    "prev_sum = cur_sum\n",
    "\n",
    "inclusion_criteria.append(filt)\n",
    "inclusion_criteria_names.append('excludes_treated_not_confirmed')\n",
    "feature_sets.append(feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marginal analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# Set treatment='confirmed' to use time to confirmation\n",
    "# as the exposure variable and treatment='abx' to \n",
    "# use time to antibiotics as the exposure variable\n",
    "################################################\n",
    "treatment = \"confirmed\"\n",
    "\n",
    "tta_col = \"hours_alert_to_abx\"\n",
    "\n",
    "# number of bootstrap replicates\n",
    "n_bs_samples = 5001\n",
    "\n",
    "res_dicts = {}\n",
    "for filt,filt_name,feats in zip(inclusion_criteria,inclusion_criteria_names,feature_sets):\n",
    "    print(f'**** Cohort: {filt_name} ****')\n",
    "    X,T,Y,feature_cols_aug = get_features(data,filt,tta_col,feats,ftype='first')\n",
    "    print()\n",
    "    \n",
    "    res_dict = {}\n",
    "    res_dict['X'] = X\n",
    "    res_dict['Y'] = Y\n",
    "    res_dict['T'] = T\n",
    "    res_dict['filter'] = filt\n",
    "    res_dict['data'] = data\n",
    "    \n",
    "    \n",
    "    # SOFA 72\n",
    "    if treatment == \"abx\":\n",
    "        A = 1*(T > 3)\n",
    "    elif treatment == \"confirmed\":\n",
    "        A = 1-1*data[filt].confirmed_in_3_hrs.values.reshape((X.shape[0],1))\n",
    "    XA = np.hstack((X,A))\n",
    "    print(f'SOFA @ 72:')\n",
    "    for i,t in enumerate([72]):\n",
    "        S = data[filt][f'sofa_{t}'].values\n",
    "        t_mask = ~(data[filt].discharge_type.isin(['censored','hospice']))\n",
    "        fmask = ~np.all(XA[t_mask]==0,axis=0)\n",
    "        disc_ols_res = sm.OLS(S[t_mask],XA[t_mask][:,fmask]).fit(cov_type='HC1')\n",
    "        pe = disc_ols_res.params[-1]\n",
    "        ci = disc_ols_res.conf_int()[-1]\n",
    "        p = disc_ols_res.pvalues[-1]\n",
    "        print(f\"{pe:0.2f} [{ci[0]:.2f} - {ci[1]:.2f}] p={p:0.3f}\")\n",
    "        res_dict['sofa_at_72'] = {'point_estimate':pe,'confidence_interval':ci,'p_value':p}\n",
    "        \n",
    "    print()\n",
    "    \n",
    "    print(f'Length of stay:')\n",
    "    los_mask = Y==0\n",
    "    LOS = (data[filt].hours_alert_to_discharge.values)\n",
    "    fmask = ~np.all(XA[los_mask]==0,axis=0)\n",
    "    disc_qr_res = sm.QuantReg(LOS[los_mask],XA[los_mask][:,fmask]).fit(q=0.5,max_iter=5000)\n",
    "    pe = disc_qr_res.params[-1]\n",
    "    ci = disc_qr_res.conf_int()[-1]\n",
    "    p = disc_qr_res.pvalues[-1]\n",
    "    print(f\"{pe:0.2f} [{ci[0]:.2f} - {ci[1]:.2f}] p={p:0.3f}\")\n",
    "    \n",
    "    res_dict['length_of_stay'] = {'point_estimate':pe,'confidence_interval':ci,'p_value':p}\n",
    "    print()\n",
    "    \n",
    "    bs_res = np.zeros(n_bs_samples)\n",
    "    ipw_res = np.zeros((n_bs_samples,2))\n",
    "    np.random.seed(5)\n",
    "    preds = np.zeros((n_bs_samples,2,Y.shape[0]))\n",
    "    XA = np.hstack((X,A))\n",
    "    for s in tqdm(list(range(n_bs_samples)),leave=False):\n",
    "        if s == 0:\n",
    "            idxs = np.arange(Y.shape[0])\n",
    "        else:\n",
    "            idxs = np.random.choice(Y.shape[0],Y.shape[0],replace=True)\n",
    "        XA_s = XA[idxs]\n",
    "        Y_s = Y[idxs]\n",
    "        lr = LogisticRegression(C=1000,solver='liblinear').fit(XA_s,Y_s)\n",
    "        preds_s = np.zeros((2,Y.shape[0]))\n",
    "        for a in range(2):\n",
    "            XA_s_int = XA_s.copy()\n",
    "            XA_s_int[:,-1] = a\n",
    "            preds[s,a] = lr.predict_proba(XA_s_int)[:,1]\n",
    "        bs_res[s] = 1 - np.mean(preds[s,0])/np.mean(preds[s,1])\n",
    "\n",
    "    print('Mortality:')\n",
    "    print((np.mean(preds[0,1]),np.mean(preds[0,0])))\n",
    "    rds = np.mean(preds[:,1] - preds[:,0],axis=1)\n",
    "    rd_ci = np.percentile(rds[1:],[2.5,97.5])\n",
    "    rd_p = 2*min(np.mean(rds[1:] > 0),np.mean(rds[1:] < 0))\n",
    "    print(f\"RD: {100*rds[0]:.2f}% [{100*rd_ci[0]:.2f}% - {100*rd_ci[1]:.2f}%] p={rd_p:.3f}\")\n",
    "    rrs = 1-np.mean(preds[:,0],axis=1)/np.mean(preds[:,1],axis=1)\n",
    "    rr_ci = np.percentile(rrs[1:],[2.5,97.5])\n",
    "    rr_p = 2*min(np.mean(rrs[1:] > 0),np.mean(rrs[1:] < 0))\n",
    "    print(f\"Rel red: {100*rrs[0]:.2f}% [{100*rr_ci[0]:.2f}% - {100*rr_ci[1]:.2f}%] p={rr_p:.3f}\")\n",
    "    print()\n",
    "    \n",
    "    res_dict['mortality'] = {'bootstrap_point_estimates':preds.mean(-1)}\n",
    "    \n",
    "    res_dicts[filt_name] = res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_col = \"hours_alert_to_abx\"\n",
    "\n",
    "# number of bootstrap replicates\n",
    "n_bs_samples = 5001\n",
    "\n",
    "for filt,filt_name,feats in zip(inclusion_criteria,inclusion_criteria_names,feature_sets):\n",
    "    print(f'**** Cohort: {filt_name} ****')\n",
    "    X,T,Y,feature_cols_aug = get_features(data,filt,tta_col,feats,ftype='first')\n",
    "    print()\n",
    "    \n",
    "    # SOFA 72\n",
    "    XT = np.hstack((X,T))\n",
    "    print(f'Per hour mortality:')\n",
    "    \n",
    "    res_dicts[filt_name]['per_hour_mortality'] = {}\n",
    "    for i,t in enumerate([6,12,24]):\n",
    "        print(f'** tmax = {t} **')\n",
    "        res = np.zeros(n_bs_samples)\n",
    "        for s in tqdm(list(range(n_bs_samples)),leave=False):\n",
    "            if s == 0:\n",
    "                idxs = np.arange(X.shape[0])\n",
    "            else:\n",
    "                idxs = np.random.choice(X.shape[0],X.shape[0],replace=True)\n",
    "                \n",
    "            XT_s = XT[idxs]\n",
    "            Y_s = Y[idxs]\n",
    "            T_s = T[idxs]\n",
    "            t_mask = T_s[:,0] <= t\n",
    "            fmask = ~np.all(XT_s[t_mask]==0,axis=0)\n",
    "            logit_res = LogisticRegression(C=1000,solver='liblinear').fit(XT_s[t_mask][:,fmask],Y_s[t_mask])\n",
    "            res[s] = np.exp(logit_res.coef_[0,-1])\n",
    "            \n",
    "        pe = res[0]\n",
    "        ci = np.percentile(res[1:],[2.5,97.5])\n",
    "        p = 2*min(np.mean(res[1:] > 1),np.mean(res[1:] < 1))\n",
    "        print(f\"{pe:0.2f} [{ci[0]:.2f} - {ci[1]:.2f}] p={p:0.3f}\")\n",
    "        \n",
    "        res_dicts[filt_name]['per_hour_mortality'][f'bootstrap_point_estimates_{t}'] = res\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save out results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filt_name in inclusion_criteria_names:\n",
    "    with open(f'/home/radams47/trews_deployment_analysis/output/{filt_name}_{treatment}_marginal_results.pkl','wb') as f:\n",
    "        pk.dump(res_dicts[filt_name],f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unadjusted outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filt,filt_name,feats in zip(inclusion_criteria,inclusion_criteria_names,feature_sets):\n",
    "    print(f'**** Cohort: {filt_name} ****')\n",
    "    X,T,Y,feature_cols_aug = get_features(data,filt,tta_col,feats,ftype='first')\n",
    "    print()\n",
    "    \n",
    "    # SOFA 72\n",
    "    XT = np.hstack((X,T))\n",
    "    if treatment == \"abx\":\n",
    "        A = T[:,0] <= 3\n",
    "#     A = 1*(~((pd.to_datetime(data[filt].eval_tsp) - data[filt].alert_tsp) <= pd.Timedelta(3,'h'))).values.reshape((X.shape[0],1))\n",
    "    elif treatment == \"confirmed\":\n",
    "        A = data[filt].confirmed_in_3_hrs.values==1\n",
    "    print('N')\n",
    "    print(f'{np.sum(A==1)} | {np.sum(A==0)}')\n",
    "    print()\n",
    "    \n",
    "    print('Mortality')\n",
    "    print(f'{np.sum(Y[(A==1)])} ({np.mean(Y[(A==1)]):.1%}) | {np.sum(Y[(A==0)])} ({np.mean(Y[(A==0)]):.1%})')\n",
    "    print()\n",
    "\n",
    "    print('SOFA @ 72')\n",
    "    t_mask = ~(data[filt].discharge_type.isin(['censored','hospice']))\n",
    "    S = data[filt][f'sofa_72'].values - data[filt][f'sofa'].values\n",
    "    print(f'{np.mean(S[t_mask&(A==1)]):.1f} ± {np.std(S[t_mask&(A==1)]):.1f} | {np.mean(S[t_mask&(A==0)]):.1f} ± {np.std(S[t_mask&(A==0)]):.1f}')\n",
    "    print()\n",
    "\n",
    "    print('LOS')\n",
    "    los_mask = Y==0\n",
    "    LOS = (data[filt].hours_alert_to_discharge.values)\n",
    "    m1 = np.median(LOS[los_mask&(A==1)])\n",
    "    lq1 = np.quantile(LOS[los_mask&(A==1)],0.25)\n",
    "    uq1 = np.quantile(LOS[los_mask&(A==1)],0.75)\n",
    "    m0 = np.median(LOS[los_mask&(A==0)])\n",
    "    lq0 = np.quantile(LOS[los_mask&(A==0)],0.25)\n",
    "    uq0 = np.quantile(LOS[los_mask&(A==0)],0.75)\n",
    "    print(f'{m1:.0f} ({lq1:.0f} - {uq1:.0f}) | {m0:.0f} ({lq0:.0f} - {uq0:.0f})')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerun using IPTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import robustats as rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment = \"confirmed\"\n",
    "tta_col = \"hours_alert_to_abx\"\n",
    "n_bs_samples = 11\n",
    "\n",
    "iptw_results = {}\n",
    "for filt,filt_name,feats in zip(inclusion_criteria,inclusion_criteria_names,feature_sets):\n",
    "    print(f'**** Cohort: {filt_name} ****')\n",
    "    X,T,Y,feature_cols_aug = get_features(data,filt,tta_col,feats,ftype='first')\n",
    "    print()\n",
    "    \n",
    "    # SOFA 72\n",
    "    if treatment == \"abx\":\n",
    "        A = 1*(T > 3)\n",
    "    elif treatment == \"confirmed\":\n",
    "        A = 1-1*data[filt].confirmed_in_3_hrs.values.reshape((X.shape[0],1))\n",
    "    XA = np.hstack((X,A))\n",
    "    \n",
    "    LOS = (data[filt].hours_alert_to_discharge.values)\n",
    "    los_mask = Y==0\n",
    "    \n",
    "    S = data[filt][f'sofa_72'].values\n",
    "    s_mask = ~(data[filt].discharge_type.isin(['censored','hospice'])).values\n",
    "    \n",
    "    res_d = {\n",
    "        'sofa':np.zeros((n_bs_samples,2)),\n",
    "        'los':np.zeros((n_bs_samples,2)),\n",
    "        'mortality':np.zeros((n_bs_samples,2)),\n",
    "    }\n",
    "    \n",
    "    for s in tqdm(list(range(n_bs_samples)),leave=False):\n",
    "        if s == 0:\n",
    "            idxs = np.arange(Y.shape[0])\n",
    "        else:\n",
    "            idxs = np.random.choice(Y.shape[0],Y.shape[0],replace=True)\n",
    "        X_s = X[idxs]\n",
    "        A_s = A[idxs][:,0]\n",
    "        Y_s = Y[idxs]\n",
    "        S_s = S[idxs]\n",
    "        LOS_s = LOS[idxs]\n",
    "        s_mask_s = s_mask[idxs]\n",
    "        los_mask_s = los_mask[idxs]\n",
    "        \n",
    "        treatment_model = LogisticRegression(C=1000,solver='liblinear').fit(X_s,A_s)\n",
    "        \n",
    "        pa = treatment_model.predict_proba(X_s)[:,1]\n",
    "        bnds = np.quantile(pa,[0.01,0.99])\n",
    "        pa = np.clip(pa,a_min=bnds[0],a_max=bnds[1])\n",
    "        \n",
    "        ey0 = np.sum(Y_s*(1-A_s)/(1-pa))/np.sum((1-A_s)/(1-pa))\n",
    "        ey1 = np.sum(Y_s*A_s/pa)/np.sum(A_s/pa)\n",
    "        \n",
    "        res_d['mortality'][s,0] = ey0\n",
    "        res_d['mortality'][s,1] = ey1\n",
    "        \n",
    "        es0 = np.sum(S_s*(1-A_s)*s_mask_s/(1-pa))/np.sum((1-A_s)*s_mask_s/(1-pa))\n",
    "        es1 = np.sum(S_s*A_s*s_mask_s/pa)/np.sum(A_s*s_mask_s/pa)\n",
    "        \n",
    "        res_d['sofa'][s,0] = es0\n",
    "        res_d['sofa'][s,1] = es1\n",
    "        \n",
    "        mlos0 = rs.weighted_median(LOS_s[los_mask_s & (A_s==0)],1/(1-pa[los_mask_s & (A_s==0)]))\n",
    "        mlos1 = rs.weighted_median(LOS_s[los_mask_s & (A_s==1)],1/(pa[los_mask_s & (A_s==1)]))\n",
    "    \n",
    "        res_d['los'][s,0] = mlos0\n",
    "        res_d['los'][s,1] = mlos1\n",
    "        \n",
    "    rds = res_d[\"sofa\"][:,1] - res_d[\"sofa\"][:,0]\n",
    "    ci = np.percentile(rds,[2.5,97.5])\n",
    "    print(f'IPTW SOFA: {rds[0]:.2f} [{ci[0]:.2f}, {ci[1]:.2f}]')\n",
    "    \n",
    "    rds = res_d[\"los\"][:,1] - res_d[\"los\"][:,0]\n",
    "    ci = np.percentile(rds,[2.5,97.5])\n",
    "    print(f'IPTW LOS: {rds[0]:.1f} [{ci[0]:.1f}, {ci[1]:.1f}]')\n",
    "    \n",
    "    rds = res_d[\"mortality\"][:,1] - res_d[\"mortality\"][:,0]\n",
    "    ci = np.percentile(rds,[2.5,97.5])\n",
    "    print(f'IPTW mortality: {rds[0]:.2%} [{ci[0]:.2%}, {ci[1]:.2%}]')\n",
    "    \n",
    "    iptw_results[filt_name] = res_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save out IPTW results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filt_name in inclusion_criteria_names:\n",
    "    with open(f'/home/radams47/trews_deployment_analysis/output/{filt_name}_{treatment}_iptw_results.pkl','wb') as f:\n",
    "        pk.dump(iptw_results[filt_name],f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-risk analysis\n",
    "Main analysis stratified by risk group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tta_col = \"hours_alert_to_abx\"\n",
    "\n",
    "# number of bootstrap replicates\n",
    "n_bs_samples = 5001\n",
    "\n",
    "high_risk_res_dicts = {}\n",
    "\n",
    "for filt,filt_name,feats in zip(inclusion_criteria,inclusion_criteria_names,feature_sets):\n",
    "    print(f'**** Cohort: {filt_name} ****')\n",
    "    X,T,Y,feature_cols_aug = get_features(data,filt,tta_col,feats,ftype='first')\n",
    "    print()\n",
    "    \n",
    "    res_dict = {}\n",
    "    res_dict['X'] = X\n",
    "    res_dict['Y'] = Y\n",
    "    res_dict['T'] = T\n",
    "    res_dict['filter'] = filt\n",
    "    res_dict['data'] = data\n",
    "    \n",
    "    Xr,_,_,_ = get_features(data,filt,tta_col,feature_cols,ftype='first')\n",
    "    disc = feat_kbd.transform(data[filt][dfeats])\n",
    "    s = np.zeros((2,X.shape[0]))\n",
    "    for t in range(2):\n",
    "        T_test = t*np.ones(T.shape)\n",
    "        XT_test = np.hstack((Xr,disc,Xr*T_test,disc*T_test))\n",
    "        s[t] = mort_model.predict_proba(XT_test)[:,1]\n",
    "\n",
    "    scores = 1 - s[0]/s[1]\n",
    "\n",
    "    risk_features = kbd.transform(scores.reshape((X.shape[0],1)))\n",
    "    print(np.sum(risk_features,axis=0))\n",
    "    print()\n",
    "    \n",
    "    res_dict['risk_scores'] = scores\n",
    "    res_dict['risk_features'] = risk_features\n",
    "    \n",
    "    for rg in range(nrg):\n",
    "        res_dict[f'risk_group_{rg}'] = {}\n",
    "        \n",
    "        print(f'** Risk group: {rg} **')\n",
    "        rmask = risk_features[:,rg] == 1\n",
    "        \n",
    "        # SOFA 72\n",
    "        if treatment == \"abx\":\n",
    "            A = 1*(T > 3)\n",
    "        elif treatment == \"confirmed\":\n",
    "            A = 1-1*data[filt].confirmed_in_3_hrs.values.reshape((X.shape[0],1))\n",
    "        XA = np.hstack((X,A))\n",
    "        print(f'SOFA @ 72:')\n",
    "        for i,t in enumerate([72]):\n",
    "            S = data[filt][f'sofa_{t}'].values\n",
    "            t_mask = ~(data[filt].discharge_type.isin(['censored','hospice']))\n",
    "            fmask = ~np.all(XA[t_mask]==0,axis=0)\n",
    "            disc_ols_res = sm.OLS(S[t_mask&rmask],XA[t_mask&rmask][:,fmask]).fit(cov_type='HC1')\n",
    "            pe = disc_ols_res.params[-1]\n",
    "            ci = disc_ols_res.conf_int()[-1]\n",
    "            p = disc_ols_res.pvalues[-1]\n",
    "            print(f\"{pe:0.2f} [{ci[0]:.2f} - {ci[1]:.2f}] p={p:0.3f}\")\n",
    "            res_dict[f'risk_group_{rg}']['sofa_at_72'] = {'point_estimate':pe,'confidence_interval':ci,'p_value':p}\n",
    "        print()\n",
    "\n",
    "        print(f'Length of stay:')\n",
    "        los_mask = Y==0\n",
    "        LOS = (data[filt].hours_alert_to_discharge.values)\n",
    "        fmask = ~np.all(XA[los_mask&rmask]==0,axis=0)\n",
    "        disc_qr_res = sm.QuantReg(LOS[los_mask&rmask],XA[los_mask&rmask][:,fmask]).fit(q=0.5,max_iter=5000)\n",
    "        pe = disc_qr_res.params[-1]\n",
    "        ci = disc_qr_res.conf_int()[-1]\n",
    "        p = disc_qr_res.pvalues[-1]\n",
    "        print(f\"{pe:0.2f} [{ci[0]:.2f} - {ci[1]:.2f}] p={p:0.3f}\")\n",
    "        res_dict[f'risk_group_{rg}']['length_of_stay'] = {'point_estimate':pe,'confidence_interval':ci,'p_value':p}\n",
    "\n",
    "        bs_res = np.zeros(n_bs_samples)\n",
    "        np.random.seed(5)\n",
    "        preds = np.zeros((n_bs_samples,2,rmask.sum()))\n",
    "        XA = np.hstack((X,A))\n",
    "        for s in tqdm(list(range(n_bs_samples)),leave=False):\n",
    "            if s == 0:\n",
    "                idxs = np.arange(rmask.sum())\n",
    "            else:\n",
    "                idxs = np.random.choice(rmask.sum(),rmask.sum(),replace=True)\n",
    "            XA_s = XA[rmask][idxs]\n",
    "            Y_s = Y[rmask][idxs]\n",
    "            lr = LogisticRegression(C=1000,solver='liblinear').fit(XA_s,Y_s)\n",
    "            for a in range(2):\n",
    "                XA_s_int = XA_s.copy()\n",
    "                XA_s_int[:,-1] = a\n",
    "                preds[s,a] = lr.predict_proba(XA_s_int)[:,1]\n",
    "            bs_res[s] = 1 - np.mean(preds[s,0])/np.mean(preds[s,1])\n",
    "\n",
    "        print('Mortality:')\n",
    "        print((np.mean(preds[0,1]),np.mean(preds[0,0])))\n",
    "        rds = np.mean(preds[:,1] - preds[:,0],axis=1)\n",
    "        rd_ci = np.percentile(rds[1:],[2.5,97.5])\n",
    "        rd_p = 2*min(np.mean(rds[1:] > 0),np.mean(rds[1:] < 0))\n",
    "        print(f\"RD: {100*rds[0]:.2f}% [{100*rd_ci[0]:.2f}% - {100*rd_ci[1]:.2f}%] p={rd_p:.3f}\")\n",
    "        rrs = 1-np.mean(preds[:,0],axis=1)/np.mean(preds[:,1],axis=1)\n",
    "        rr_ci = np.percentile(rrs[1:],[2.5,97.5])\n",
    "        rr_p = 2*min(np.mean(rrs[1:] > 0),np.mean(rrs[1:] < 0))\n",
    "        print(f\"Rel red: {100*rrs[0]:.2f}% [{100*rr_ci[0]:.2f}% - {100*rr_ci[1]:.2f}%] p={rr_p:.3f}\")\n",
    "        print()\n",
    "        \n",
    "        res_dict[f'risk_group_{rg}']['mortality'] = {'bootstrap_point_estimates':preds.mean(-1)}\n",
    "    \n",
    "    high_risk_res_dicts[filt_name] = res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tta_col = \"hours_alert_to_abx\"\n",
    "\n",
    "for filt,filt_name,feats in zip(inclusion_criteria,inclusion_criteria_names,feature_sets):\n",
    "    print(f'**** Cohort: {filt_name} ****')\n",
    "    X,T,Y,feature_cols_aug = get_features(data,filt,tta_col,feats,ftype='first')\n",
    "    print()\n",
    "    \n",
    "    Xr,_,_,_ = get_features(data,filt,tta_col,feature_cols,ftype='first')\n",
    "    disc = feat_kbd.transform(data[filt][dfeats])\n",
    "    s = np.zeros((2,X.shape[0]))\n",
    "    for t in range(2):\n",
    "        T_test = t*np.ones(T.shape)\n",
    "        XT_test = np.hstack((Xr,disc,Xr*T_test,disc*T_test))\n",
    "        s[t] = mort_model.predict_proba(XT_test)[:,1]\n",
    "\n",
    "    scores = 1 - s[0]/s[1]\n",
    "\n",
    "    risk_features = kbd.transform(scores.reshape((X.shape[0],1)))\n",
    "    print(np.sum(risk_features,axis=0))\n",
    "    print()\n",
    "    for rg in range(nrg):\n",
    "        high_risk_res_dicts[filt_name][f'risk_group_{rg}']['per_hour_mortality'] = {}\n",
    "        \n",
    "        print(f'** Risk group: {rg} **')\n",
    "        rmask = risk_features[:,rg] == 1\n",
    "        \n",
    "        # SOFA 72\n",
    "        XT = np.hstack((X,T))\n",
    "        for i,t in enumerate([6,12,24]):\n",
    "            \n",
    "            print(f'** tmax = {t} **')\n",
    "            res = np.zeros(n_bs_samples)\n",
    "            for s in tqdm(list(range(n_bs_samples)),leave=False):\n",
    "                if s == 0:\n",
    "                    idxs = np.arange(rmask.sum())\n",
    "                else:\n",
    "                    idxs = np.random.choice(rmask.sum(),rmask.sum(),replace=True)\n",
    "\n",
    "                XT_s = XT[rmask][idxs]\n",
    "                Y_s = Y[rmask][idxs]\n",
    "                T_s = T[rmask][idxs]\n",
    "                t_mask = T_s[:,0] <= t\n",
    "                fmask = ~np.all(XT_s[t_mask]==0,axis=0)\n",
    "                logit_res = LogisticRegression(C=1000,solver='liblinear').fit(XT_s[t_mask][:,fmask],Y_s[t_mask])\n",
    "                res[s] = np.exp(logit_res.coef_[0,-1])\n",
    "\n",
    "            pe = res[0]\n",
    "            ci = np.percentile(res[1:],[2.5,97.5])\n",
    "            p = 2*min(np.mean(res[1:] > 1),np.mean(res[1:] < 1))\n",
    "            print(f\"{pe:0.2f} [{ci[0]:.2f} - {ci[1]:.2f}] p={p:0.3f}\")\n",
    "            high_risk_res_dicts[filt_name][f'risk_group_{rg}']['per_hour_mortality'][f'bootstrap_point_estimates_{t}'] = res\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save out risk-stratified results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for filt_name in inclusion_criteria_names:\n",
    "    with open(f'/home/radams47/trews_deployment_analysis/output/{filt_name}_{treatment}_high_risk_results.pkl','wb') as f:\n",
    "        pk.dump(high_risk_res_dicts[filt_name],f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unadjusted outcomes by risk group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for filt,filt_name,feats in zip(inclusion_criteria,inclusion_criteria_names,feature_sets):\n",
    "    print(f'**** Cohort: {filt_name} ****')\n",
    "    X,T,Y,feature_cols_aug = get_features(data,filt,tta_col,feats,ftype='first')\n",
    "    print()\n",
    "    \n",
    "    Xr,_,_,_ = get_features(data,filt,tta_col,feature_cols,ftype='first')\n",
    "    disc = feat_kbd.transform(data[filt][dfeats])\n",
    "    s = np.zeros((2,X.shape[0]))\n",
    "    for t in range(2):\n",
    "        T_test = t*np.ones(T.shape)\n",
    "        XT_test = np.hstack((Xr,disc,Xr*T_test,disc*T_test))\n",
    "        s[t] = mort_model.predict_proba(XT_test)[:,1]\n",
    "\n",
    "    scores = 1 - s[0]/s[1]\n",
    "\n",
    "    risk_features = kbd.transform(scores.reshape((X.shape[0],1)))\n",
    "    print(np.sum(risk_features,axis=0))\n",
    "    print()\n",
    "    for rg in range(nrg):\n",
    "        print(f'** Risk group: {rg} **')\n",
    "        rmask = risk_features[:,rg] == 1\n",
    "        \n",
    "        # SOFA 72\n",
    "        XT = np.hstack((X,T))\n",
    "        if treatment == \"abx\":\n",
    "            A = T[:,0] <= 3\n",
    "        elif treatment == \"confirmed\":\n",
    "            A = data[filt].confirmed_in_3_hrs.values==1\n",
    "        print('N')\n",
    "        print(f'{np.sum(rmask&(A==1))} | {np.sum(rmask&(A==0))}')\n",
    "        print()\n",
    "        \n",
    "        print('Mortality')\n",
    "        print(f'{np.sum(Y[rmask&(A==1)])} ({np.mean(Y[rmask&(A==1)]):.1%}) | {np.sum(Y[rmask&(A==0)])} ({np.mean(Y[rmask&(A==0)]):.1%})')\n",
    "        print()\n",
    "        \n",
    "        print('SOFA @ 72')\n",
    "        t_mask = ~(data[filt].discharge_type.isin(['censored','hospice']))\n",
    "        S = data[filt][f'sofa_72'].values - data[filt][f'sofa'].values\n",
    "        print(f'{np.mean(S[rmask&t_mask&(A==1)]):.1f} ± {np.std(S[rmask&t_mask&(A==1)]):.1f} | {np.mean(S[rmask&t_mask&(A==0)]):.1f} ± {np.std(S[rmask&t_mask&(A==0)]):.1f}')\n",
    "        print()\n",
    "        \n",
    "        print('LOS')\n",
    "        los_mask = Y==0\n",
    "        LOS = (data[filt].hours_alert_to_discharge.values)\n",
    "        m1 = np.median(LOS[rmask&los_mask&(A==1)])\n",
    "        lq1 = np.quantile(LOS[rmask&los_mask&(A==1)],0.25)\n",
    "        uq1 = np.quantile(LOS[rmask&los_mask&(A==1)],0.75)\n",
    "        m0 = np.median(LOS[rmask&los_mask&(A==0)])\n",
    "        lq0 = np.quantile(LOS[rmask&los_mask&(A==0)],0.25)\n",
    "        uq0 = np.quantile(LOS[rmask&los_mask&(A==0)],0.75)\n",
    "        print(f'{m1:.0f} ({lq1:.0f} - {uq1:.0f}) | {m0:.0f} ({lq0:.0f} - {uq0:.0f})')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save out risk strata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_risk_res_dicts['all'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_filt = high_risk_res_dicts['all']['filter']\n",
    "rf = high_risk_res_dicts['all']['risk_features']\n",
    "X = high_risk_res_dicts['all']['X']\n",
    "risk_strata = data[sv_filt][['enc_id','admission_tsp','alert_tsp','discharge_tsp','first_antibiotics_tsp','first_antibiotics_order_tsp','death_in_hospital','discharge_type','sofa']].copy()\n",
    "risk_strata['dataset_id'] = 77\n",
    "for i,rg in enumerate(['low','mid','high']):\n",
    "    risk_strata[f'{rg}_risk'] = rf[:,i]\n",
    "    \n",
    "for c,f in enumerate(feature_cols_aug[1:]):\n",
    "    risk_strata[f] = X[:,c+1]\n",
    "    \n",
    "risk_strata.to_csv('/edata/clarity_extracts/processed_features/risk_strata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
